{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aquatic-retreat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T01:47:41.936884Z",
     "start_time": "2021-05-11T01:47:41.266707Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polar-yorkshire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:06.553820Z",
     "start_time": "2021-05-11T05:56:05.959239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('jeopardy-topic-modeling/JEOPARDY_CSV.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greatest-portugal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:08.207113Z",
     "start_time": "2021-05-11T05:56:08.191876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jeopardy!', 'Double Jeopardy!', 'Final Jeopardy!', 'Tiebreaker'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[' Round'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "republican-garage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:09.120303Z",
     "start_time": "2021-05-11T05:56:09.101065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3631"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[' Round'] == 'Final Jeopardy!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-morocco",
   "metadata": {},
   "source": [
    "### Clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "north-problem",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:10.316975Z",
     "start_time": "2021-05-11T05:56:10.314130Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns = ['show_id', 'date', 'round', 'category', 'value', 'question', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "funky-orientation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:19.844397Z",
     "start_time": "2021-05-11T05:56:19.841924Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gentle-society",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:23.405973Z",
     "start_time": "2021-05-11T05:56:23.403467Z"
    }
   },
   "outputs": [],
   "source": [
    "#df[df.value == 'None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intimate-slide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:34.459190Z",
     "start_time": "2021-05-11T05:56:34.440162Z"
    }
   },
   "outputs": [],
   "source": [
    "df['value'].replace({'None': '$0'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "streaming-supplement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:35.566122Z",
     "start_time": "2021-05-11T05:56:35.437823Z"
    }
   },
   "outputs": [],
   "source": [
    "df['value'] = df['value'].apply(lambda x: int(x[1:].replace(',',''))) #remove $ remove comma and convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "typical-wallet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:56:55.792101Z",
     "start_time": "2021-05-11T05:56:55.674996Z"
    }
   },
   "outputs": [],
   "source": [
    "df['year'] = df['date'].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "attractive-chain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:57:08.069770Z",
     "start_time": "2021-05-11T05:57:08.060006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>date</th>\n",
       "      <th>round</th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   show_id        date      round                         category  value  \\\n",
       "0     4680  2004-12-31  Jeopardy!                          HISTORY    200   \n",
       "1     4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES    200   \n",
       "2     4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...    200   \n",
       "3     4680  2004-12-31  Jeopardy!                 THE COMPANY LINE    200   \n",
       "4     4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES    200   \n",
       "\n",
       "                                            question      answer  year  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  2004  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  2004  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  2004  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  2004  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  2004  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fifteen-corruption",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:58:32.414514Z",
     "start_time": "2021-05-11T05:58:32.398603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Jeopardy!', 'Double Jeopardy!', 'Final Jeopardy!', 'Tiebreaker'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['round'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-dividend",
   "metadata": {},
   "source": [
    "### Explore categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "incredible-sympathy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T06:17:15.400790Z",
     "start_time": "2021-05-11T06:17:15.361779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEFORE & AFTER             547\n",
       "SCIENCE                    519\n",
       "LITERATURE                 496\n",
       "AMERICAN HISTORY           418\n",
       "POTPOURRI                  401\n",
       "WORLD HISTORY              377\n",
       "WORD ORIGINS               371\n",
       "COLLEGES & UNIVERSITIES    351\n",
       "HISTORY                    349\n",
       "SPORTS                     342\n",
       "U.S. CITIES                339\n",
       "WORLD GEOGRAPHY            338\n",
       "BODIES OF WATER            327\n",
       "ANIMALS                    324\n",
       "STATE CAPITALS             314\n",
       "BUSINESS & INDUSTRY        311\n",
       "ISLANDS                    301\n",
       "WORLD CAPITALS             300\n",
       "U.S. GEOGRAPHY             299\n",
       "RELIGION                   297\n",
       "OPERA                      294\n",
       "SHAKESPEARE                294\n",
       "LANGUAGES                  284\n",
       "BALLET                     282\n",
       "TELEVISION                 281\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-tuition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-vanilla",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "entitled-cincinnati",
   "metadata": {},
   "source": [
    "## Attempt at running LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "temporal-return",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:12:49.404848Z",
     "start_time": "2021-05-11T18:12:49.401269Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "convinced-classroom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:13:19.167061Z",
     "start_time": "2021-05-11T18:13:18.559847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuamajano/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "expected-syntax",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:35:49.655312Z",
     "start_time": "2021-05-11T18:35:49.649176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1179, 8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df[df.year==1984]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "capable-given",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:35:52.260570Z",
     "start_time": "2021-05-11T18:35:52.256924Z"
    }
   },
   "outputs": [],
   "source": [
    "data_text = test[['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cosmetic-battlefield",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:35:52.907292Z",
     "start_time": "2021-05-11T18:35:52.900491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>What the \"D.C.\" stands for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>Color of a lucky \"letter day\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>While 2 of its wheels head toward produce, 1 g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>Varieties include Chinese &amp; Dijon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>Turned down to be an \"Our Gang\" member, she be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question\n",
       "8544                         What the \"D.C.\" stands for\n",
       "8545                      Color of a lucky \"letter day\"\n",
       "8546  While 2 of its wheels head toward produce, 1 g...\n",
       "8547                  Varieties include Chinese & Dijon\n",
       "8548  Turned down to be an \"Our Gang\" member, she be..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "immune-mandate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:36:09.129747Z",
     "start_time": "2021-05-11T18:36:09.127851Z"
    }
   },
   "outputs": [],
   "source": [
    "#data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "funky-samba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:36:16.974631Z",
     "start_time": "2021-05-11T18:36:16.970373Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'the', '\"d.c.\"', 'stands', 'for']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.iloc[0]['question'].lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "blank-tower",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:37:22.175748Z",
     "start_time": "2021-05-11T18:37:20.641574Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(data_text)):\n",
    "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
    "    data_text.iloc[idx]['question'] = [word for word in data_text.iloc[idx]['question'].lower().split(' ') if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abstract-carter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:37:33.301382Z",
     "start_time": "2021-05-11T18:37:33.294398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>[\"d.c.\", stands]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>[color, lucky, \"letter, day\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>[2, wheels, head, toward, produce,, 1, goes, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>[varieties, include, chinese, &amp;, dijon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>[turned, \"our, gang\", member,, became, top, bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question\n",
       "8544                                   [\"d.c.\", stands]\n",
       "8545                      [color, lucky, \"letter, day\"]\n",
       "8546  [2, wheels, head, toward, produce,, 1, goes, d...\n",
       "8547            [varieties, include, chinese, &, dijon]\n",
       "8548  [turned, \"our, gang\", member,, became, top, bo..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "clear-billy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:38:32.827520Z",
     "start_time": "2021-05-11T18:38:32.824002Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_headlines = [value[0] for value in data_text.iloc[0:].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "genetic-drunk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:06:57.352807Z",
     "start_time": "2021-05-11T19:06:57.351000Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "color-party",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:39:13.013345Z",
     "start_time": "2021-05-11T18:39:12.968789Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "broad-motivation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:39:23.895091Z",
     "start_time": "2021-05-11T18:39:23.883163Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in train_headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "metropolitan-burns",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:39:42.261367Z",
     "start_time": "2021-05-11T18:39:41.456019Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "violent-hollow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T18:39:57.832465Z",
     "start_time": "2021-05-11T18:39:57.810117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>1st</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>state</td>\n",
       "      <td>name</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st</td>\n",
       "      <td>1</td>\n",
       "      <td>name</td>\n",
       "      <td>name</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>john</td>\n",
       "      <td>first</td>\n",
       "      <td>years</td>\n",
       "      <td>country</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u.s.</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>call</td>\n",
       "      <td>\"the</td>\n",
       "      <td>1st</td>\n",
       "      <td>used</td>\n",
       "      <td>played</td>\n",
       "      <td>film</td>\n",
       "      <td>number</td>\n",
       "      <td>song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"the</td>\n",
       "      <td>2</td>\n",
       "      <td>country</td>\n",
       "      <td>came</td>\n",
       "      <td>became</td>\n",
       "      <td>people</td>\n",
       "      <td>named</td>\n",
       "      <td>name</td>\n",
       "      <td>largest</td>\n",
       "      <td>&lt;a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name</td>\n",
       "      <td>called</td>\n",
       "      <td>comes</td>\n",
       "      <td>president</td>\n",
       "      <td>famous</td>\n",
       "      <td>one</td>\n",
       "      <td>term</td>\n",
       "      <td>france</td>\n",
       "      <td>world's</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>body</td>\n",
       "      <td>named</td>\n",
       "      <td>greek</td>\n",
       "      <td>first</td>\n",
       "      <td>called</td>\n",
       "      <td>3</td>\n",
       "      <td>include</td>\n",
       "      <td>name</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>state</td>\n",
       "      <td>\"the</td>\n",
       "      <td>roman</td>\n",
       "      <td>call</td>\n",
       "      <td>british</td>\n",
       "      <td>last</td>\n",
       "      <td>day</td>\n",
       "      <td>called</td>\n",
       "      <td>though</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>first</td>\n",
       "      <td>that's</td>\n",
       "      <td>people</td>\n",
       "      <td>black</td>\n",
       "      <td>home</td>\n",
       "      <td>still</td>\n",
       "      <td>number</td>\n",
       "      <td>1st</td>\n",
       "      <td>america</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>though</td>\n",
       "      <td>american</td>\n",
       "      <td>whose</td>\n",
       "      <td>best</td>\n",
       "      <td>country</td>\n",
       "      <td>crew</td>\n",
       "      <td>though</td>\n",
       "      <td>old</td>\n",
       "      <td>could</td>\n",
       "      <td>largest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>became</td>\n",
       "      <td>bird</td>\n",
       "      <td>river</td>\n",
       "      <td>africa's</td>\n",
       "      <td>city</td>\n",
       "      <td>british</td>\n",
       "      <td>president</td>\n",
       "      <td>8</td>\n",
       "      <td>war</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>time</td>\n",
       "      <td>7</td>\n",
       "      <td>tv</td>\n",
       "      <td>made</td>\n",
       "      <td>\"the</td>\n",
       "      <td>whose</td>\n",
       "      <td>city</td>\n",
       "      <td>largest</td>\n",
       "      <td>tv</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>word</td>\n",
       "      <td>one</td>\n",
       "      <td>make</td>\n",
       "      <td>movie</td>\n",
       "      <td>president</td>\n",
       "      <td>11</td>\n",
       "      <td>new</td>\n",
       "      <td>state</td>\n",
       "      <td>longest</td>\n",
       "      <td>across</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>make</td>\n",
       "      <td>3</td>\n",
       "      <td>used</td>\n",
       "      <td>&lt;a</td>\n",
       "      <td>one</td>\n",
       "      <td>instead</td>\n",
       "      <td>\"the</td>\n",
       "      <td>used</td>\n",
       "      <td>played</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>san</td>\n",
       "      <td>since</td>\n",
       "      <td>song</td>\n",
       "      <td>whose</td>\n",
       "      <td>&lt;a</td>\n",
       "      <td>though</td>\n",
       "      <td>wrote</td>\n",
       "      <td>u.s.</td>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>german</td>\n",
       "      <td>states</td>\n",
       "      <td>date</td>\n",
       "      <td>1st</td>\n",
       "      <td>english</td>\n",
       "      <td>first</td>\n",
       "      <td>name</td>\n",
       "      <td>end</td>\n",
       "      <td>red</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>comes</td>\n",
       "      <td>u.s.</td>\n",
       "      <td>east</td>\n",
       "      <td>since</td>\n",
       "      <td>type</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>state</td>\n",
       "      <td>crown</td>\n",
       "      <td>color</td>\n",
       "      <td>indians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>canada</td>\n",
       "      <td>book</td>\n",
       "      <td>film</td>\n",
       "      <td>birds</td>\n",
       "      <td>word</td>\n",
       "      <td>america</td>\n",
       "      <td>author</td>\n",
       "      <td>considered</td>\n",
       "      <td>president</td>\n",
       "      <td>sides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>president,</td>\n",
       "      <td>though</td>\n",
       "      <td>jackson</td>\n",
       "      <td>horse</td>\n",
       "      <td>called</td>\n",
       "      <td>war</td>\n",
       "      <td>words</td>\n",
       "      <td>color</td>\n",
       "      <td>2</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>highest</td>\n",
       "      <td>started</td>\n",
       "      <td>french</td>\n",
       "      <td>show</td>\n",
       "      <td>form</td>\n",
       "      <td>ball</td>\n",
       "      <td>elected</td>\n",
       "      <td>loved</td>\n",
       "      <td>never</td>\n",
       "      <td>since</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>film</td>\n",
       "      <td>make</td>\n",
       "      <td>got</td>\n",
       "      <td>blood</td>\n",
       "      <td>made</td>\n",
       "      <td>strike</td>\n",
       "      <td>colorado</td>\n",
       "      <td>party</td>\n",
       "      <td>island</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic # 01 Topic # 02 Topic # 03 Topic # 04 Topic # 05 Topic # 06  \\\n",
       "0            &        1st          &          &      state       name   \n",
       "1          1st          1       name       name          &       john   \n",
       "2         u.s.          &       call       \"the        1st       used   \n",
       "3         \"the          2    country       came     became     people   \n",
       "4         name     called      comes  president     famous        one   \n",
       "5            4       body      named      greek      first     called   \n",
       "6        state       \"the      roman       call    british       last   \n",
       "7        first     that's     people      black       home      still   \n",
       "8       though   american      whose       best    country       crew   \n",
       "9       became       bird      river   africa's       city    british   \n",
       "10        time          7         tv       made       \"the      whose   \n",
       "11        word        one       make      movie  president         11   \n",
       "12        make          3       used         <a        one    instead   \n",
       "13         san      since       song      whose         <a     though   \n",
       "14      german     states       date        1st    english      first   \n",
       "15       comes       u.s.       east      since       type          &   \n",
       "16      canada       book       film      birds       word    america   \n",
       "17  president,     though    jackson      horse     called        war   \n",
       "18     highest    started     french       show       form       ball   \n",
       "19        film       make        got      blood       made     strike   \n",
       "\n",
       "   Topic # 07  Topic # 08 Topic # 09 Topic # 10  \n",
       "0           &           &          &          &  \n",
       "1       first       years    country       made  \n",
       "2      played        film     number       song  \n",
       "3       named        name    largest         <a  \n",
       "4        term      france    world's       time  \n",
       "5           3     include       name        new  \n",
       "6         day      called     though       type  \n",
       "7      number         1st    america      first  \n",
       "8      though         old      could    largest  \n",
       "9   president           8        war     french  \n",
       "10       city     largest         tv       said  \n",
       "11        new       state    longest     across  \n",
       "12       \"the        used     played      state  \n",
       "13      wrote        u.s.       film        one  \n",
       "14       name         end        red       show  \n",
       "15      state       crown      color    indians  \n",
       "16     author  considered  president      sides  \n",
       "17      words       color          2       long  \n",
       "18    elected       loved      never      since  \n",
       "19   colorado       party     island       lake  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);\n",
    "\n",
    "get_lda_topics(lda, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-spanking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-taste",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-trout",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-combining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-beaver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-newspaper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bored-headquarters",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T19:06:48.253774Z",
     "start_time": "2021-05-11T19:06:48.251639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "#import sys\n",
    "from nltk.corpus import stopwords;\n",
    "import nltk\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "#import pickle;\n",
    "\n",
    "\n",
    "# # Loading the data\n",
    "# \n",
    "# We are using the ABC News headlines dataset. Some lines are badly formatted (very few), so we are skipping those.\n",
    "\n",
    "#We only need the Headlines_text column from the data\n",
    "data_text = data[['headline_text']];\n",
    "\n",
    "\n",
    "# We need to remove stopwords first. Casting all values to float will make it easier to iterate over.\n",
    "\n",
    "data_text = data_text.astype('str');\n",
    "\n",
    "for idx in range(len(data_text)):\n",
    "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
    "    data_text.iloc[idx]['headline_text'] = [word for word in data_text.iloc[idx]['headline_text'].split(' ') if word not in stopwords.words()];\n",
    "    \n",
    "    #print logs to monitor output\n",
    "    #if idx % 1000 == 0:\n",
    "    #    sys.stdout.write('\\rc = ' + str(idx) + ' / ' + str(len(data_text)));\n",
    "\n",
    "#save data because it takes very long to remove stop words\n",
    "#pickle.dump(data_text, open('data_text.dat', 'wb'))\n",
    "\n",
    "#get the words as an array for lda input\n",
    "train_headlines = [value[0] for value in data_text.iloc[0:].values];\n",
    "\n",
    "#number of topics we will cluster for: 10\n",
    "num_topics = 10;\n",
    "\n",
    "\n",
    "# # LDA\n",
    "# We will use the gensim library for LDA. First, we obtain a id-2-word dictionary. \n",
    "#For each headline, we will use the dictionary to obtain a mapping of the word id to their word counts. \n",
    "#The LDA model uses both of these mappings.\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(train_headlines);\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in train_headlines];\n",
    "\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics);\n",
    "\n",
    "\n",
    "# # generating LDA topics\n",
    "# \n",
    "# We will iterate over the number of topics, get the top words in each cluster and add them to a dataframe.\n",
    "\n",
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);\n",
    "\n",
    "get_lda_topics(lda, num_topics)\n",
    "\n",
    "\n",
    "\n",
    "# # NMF\n",
    "# For NMF, we need to obtain a design matrix. To improve results, I am going to apply TfIdf \n",
    "#transformation to the counts.\n",
    "\n",
    "#the count vectorizer needs string inputs, not array, so I join them with a space.\n",
    "train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
    "\n",
    "\n",
    "# Now, we obtain a Counts design matrix, for which we use SKLearnâ€™s CountVectorizer module. \n",
    "#The transformation will return a matrix of size (Documents x Features), where the value of a cell \n",
    "#is going to be the number of times the feature (word) appears in that document.\n",
    "# To reduce the size of the matrix, to speed up computation, we will set the maximum feature size \n",
    "#to 5000, which will take the top 5000 best features that can contribute to our model.\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000);\n",
    "x_counts = vectorizer.fit_transform(train_headlines_sentences);\n",
    "\n",
    "\n",
    "# Next, we set a TfIdf Transformer, and transform the counts with the model.\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False);\n",
    "x_tfidf = transformer.fit_transform(x_counts);\n",
    "\n",
    "\n",
    "# And now we normalize the TfIdf values to unit length for each row.\n",
    "\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "\n",
    "\n",
    "# And finally, obtain a NMF model, and fit it with the sentences.\n",
    "\n",
    "#obtain a NMF model.\n",
    "model = NMF(n_components=num_topics, init='nndsvd');\n",
    "\n",
    "#fit the model\n",
    "model.fit(xtfidf_norm)\n",
    "\n",
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);\n",
    "\n",
    "get_nmf_topics(model, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
